{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ab475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3101cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self,hidden_dim,embedding_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.input_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "\n",
    "        self.Ui = torch.nn.Parameter(torch.Tensor(embedding_dim,hidden_dim))\n",
    "        self.Vi = torch.nn.Parameter(torch.Tensor(hidden_dim,hidden_dim))\n",
    "        self.bi = torch.nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.Uf = torch.nn.Parameter(torch.Tensor(embedding_dim,hidden_dim))\n",
    "        self.Vf = torch.nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.bf = torch.nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.Uc = torch.nn.Parameter(torch.Tensor(embedding_dim,hidden_dim))\n",
    "        self.Vc = torch.nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.bc = torch.nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.Uo = torch.nn.Parameter(torch.Tensor(embedding_dim,hidden_dim))\n",
    "        self.Vo = torch.nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.bo = torch.nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.linear = torch.nn.Linear(hidden_dim, 2)\n",
    "\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        bs, seq_sz = x.size()\n",
    "        hidden_seq = []\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if init_states is None:\n",
    "            h_t, c_t = (\n",
    "                torch.zeros(bs, self.hidden_dim).to(x.device),\n",
    "                torch.zeros(bs, self.hidden_dim).to(x.device),\n",
    "            )\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "\n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:,t,:]\n",
    "\n",
    "            i_t = torch.sigmoid(x_t @ self.Ui + h_t @ self.Vi + self.bi)\n",
    "            f_t = torch.sigmoid(x_t @ self.Uf + h_t @ self.Vf + self.bf)\n",
    "            o_t = torch.sigmoid(x_t @ self.Uo + h_t @ self.Vo + self.bo)\n",
    "            g_t = torch.tanh(x_t @ self.Uc + h_t @ self.Vc + self.bc)\n",
    "\n",
    "            c_t = f_t * c_t + i_t * g_t \n",
    "            h_t = o_t * torch.tanh(c_t )\n",
    "\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "\n",
    "        hidden_seq = torch.cat(hidden_seq, dim = 0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        logits = self.linear(h_t)  # last timestep hidden state\n",
    "\n",
    "        return logits, hidden_seq, (h_t, c_t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96aa80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.experimental.datasets import IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49801aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000lines [00:02, 9480.63lines/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Make sure download directory exists\n",
    "os.makedirs(\".data\", exist_ok=True)\n",
    "\n",
    "from torchtext.experimental.datasets import IMDB\n",
    "\n",
    "train_dataset, test_dataset = IMDB(root=\".data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62328208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 25000\n",
      "Test size: 25000\n",
      "Example: (tensor(0), tensor([   13,  1568,    13,   246, 35468,    43,    64,   398,  1135,    92,\n",
      "            7,    37,     2,  7126,    15,  3363,    11,    60,    11,    17,\n",
      "           94,   629,    12,  6921,     3,    13,    87,   553,    15,    38,\n",
      "           94,    11,    17, 20193,    40,  1225,     3,    16,     3,  9263,\n",
      "           51,    11,   131,   780,     8,  2480,    14,   682,     4,  1575,\n",
      "          118,     6,   342,     7,   114,  1160,  3052,    13,    72,    75,\n",
      "            8,    74,    14,    19,   537,     3,     2,   121,    10,  5959,\n",
      "          194,     6,   191,  3862,   474,  1424,   766,  4314,    42,   489,\n",
      "            8,   834,   287,    61,    58,    50,   127,     3,    12,   826,\n",
      "           61,   489,     8,  1132,    47, 11859,     8,   257,    56,   441,\n",
      "            7,   669,    28,    54,     2,   863, 29737,   209,    50,   781,\n",
      "         1001,  1304,   147,    18,     2,  2675,   337,     5,  1510,  1304,\n",
      "           12,     2,  2359,  1592,     3,    12,   203,  2182,  7271,     5,\n",
      "         1919, 19586,     7, 21478,    50,    73,  4656,    28,  2381,     4,\n",
      "           61,    52,   402,    20,    47,   474,  1692,     4,  8135,     4,\n",
      "            5,   999,   347,     3,    54,  1080,    78,    50,    13,   246,\n",
      "        35468,    10,    15,  1614,   161,   587,     4,    14,    17,  1160,\n",
      "         8206,     3,    72,     4,     2,   402,     5,  1000,   145,    31,\n",
      "          175,     5,   242,   203,     4,    63,   101,    11,     9,    16,\n",
      "           29,   330,    45,    56,  6655,   100,  4461,     3,   143,    64,\n",
      "        23465,   348,   172,    11,  1574,     4,    12,   635,   402,     5,\n",
      "         1000,    31,     6,   663, 10198,    12,  3862,   437,     3,    63,\n",
      "        14516,  4441,     4,  4634,    73,  1476,     8,    57,   176,   435,\n",
      "          304,  1721,     4,    75,   402,   145,    12,    32,   114,     3,\n",
      "           13,    91, 12612,     2,  1023,    19,     2,   198,    15,   107,\n",
      "          402,   608,    12,     2,    23,    10,   608,    19,  1577,  5042,\n",
      "          251,    79,    49,     8,  1498,    85,     5,   105,   280,     8,\n",
      "           34,   608,    12,  8206,  2181,    12,   835,     3,    13,   246,\n",
      "        35468,    10,     6,    57,    23,    19,   256,  1752,     8,  2058,\n",
      "            2,  3751,     5, 18993,    25,    65,  5230,  1400,    24,     7,\n",
      "         3862,   437,     3,    22,    72,     4,    14,    23,   159,     9,\n",
      "           27,    33,    81,     7,     6,   121,     3]))\n"
     ]
    }
   ],
   "source": [
    "train_list = list(train_dataset)\n",
    "test_list = list(test_dataset)\n",
    "\n",
    "print(\"Train size:\", len(train_list))\n",
    "print(\"Test size:\", len(test_list))\n",
    "print(\"Example:\", train_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd91f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 20000\n",
      "Validation size: 5000\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Validation size:\", len(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad57f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    batch: list of tuples (label_tensor, sequence_tensor)\n",
    "    \"\"\"\n",
    "    labels = torch.tensor([entry[0].item() for entry in batch])  # shape (batch_size,)\n",
    "    sequences = [entry[1] for entry in batch]  # list of tensors\n",
    "    \n",
    "    # pad sequences to max length in this batch\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_sequences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d50c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5749421",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_list, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5fa4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max(max(seq[1].tolist()) for seq in train_list) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9aa4e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100684"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1d977a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(32,64,vocab_size)\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "learning_rate=1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d58f286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)[0]\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            current = batch * X.size(0)\n",
    "            print(f\"loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred, _, _ = model(X)\n",
    "            total_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / size\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd4b17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.690272  [    0/20000]\n",
      "loss: 0.705965  [ 1600/20000]\n",
      "loss: 0.698567  [ 3200/20000]\n",
      "loss: 0.689851  [ 4800/20000]\n",
      "loss: 0.693281  [ 6400/20000]\n",
      "loss: 0.700279  [ 8000/20000]\n",
      "loss: 0.696541  [ 9600/20000]\n",
      "loss: 0.684629  [11200/20000]\n",
      "loss: 0.698404  [12800/20000]\n",
      "loss: 0.699753  [14400/20000]\n",
      "loss: 0.697020  [16000/20000]\n",
      "loss: 0.681342  [17600/20000]\n",
      "loss: 0.686454  [19200/20000]\n",
      "Train Loss: 0.6929 | Val Loss: 0.6938 | Val Acc: 50.92%\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.681326  [    0/20000]\n",
      "loss: 0.676287  [ 1600/20000]\n",
      "loss: 0.684601  [ 3200/20000]\n",
      "loss: 0.691303  [ 4800/20000]\n",
      "loss: 0.701291  [ 6400/20000]\n",
      "loss: 0.694792  [ 8000/20000]\n",
      "loss: 0.695791  [ 9600/20000]\n",
      "loss: 0.696257  [11200/20000]\n",
      "loss: 0.697408  [12800/20000]\n",
      "loss: 0.687193  [14400/20000]\n",
      "loss: 0.685558  [16000/20000]\n",
      "loss: 0.706288  [17600/20000]\n",
      "loss: 0.698996  [19200/20000]\n",
      "Train Loss: 0.6897 | Val Loss: 0.6935 | Val Acc: 51.10%\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.698233  [    0/20000]\n",
      "loss: 0.679285  [ 1600/20000]\n",
      "loss: 0.676849  [ 3200/20000]\n",
      "loss: 0.675100  [ 4800/20000]\n",
      "loss: 0.682217  [ 6400/20000]\n",
      "loss: 0.682993  [ 8000/20000]\n",
      "loss: 0.676714  [ 9600/20000]\n",
      "loss: 0.680090  [11200/20000]\n",
      "loss: 0.672502  [12800/20000]\n",
      "loss: 0.697867  [14400/20000]\n",
      "loss: 0.729149  [16000/20000]\n",
      "loss: 0.679207  [17600/20000]\n",
      "loss: 0.674976  [19200/20000]\n",
      "Train Loss: 0.6849 | Val Loss: 0.6940 | Val Acc: 51.22%\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.717161  [    0/20000]\n",
      "loss: 0.678036  [ 1600/20000]\n",
      "loss: 0.666522  [ 3200/20000]\n",
      "loss: 0.680144  [ 4800/20000]\n",
      "loss: 0.683210  [ 6400/20000]\n",
      "loss: 0.690617  [ 8000/20000]\n",
      "loss: 0.676318  [ 9600/20000]\n",
      "loss: 0.673202  [11200/20000]\n",
      "loss: 0.709989  [12800/20000]\n",
      "loss: 0.731815  [14400/20000]\n",
      "loss: 0.675286  [16000/20000]\n",
      "loss: 0.674443  [17600/20000]\n",
      "loss: 0.675404  [19200/20000]\n",
      "Train Loss: 0.6829 | Val Loss: 0.6940 | Val Acc: 49.70%\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.675255  [    0/20000]\n",
      "loss: 0.723438  [ 1600/20000]\n",
      "loss: 0.673305  [ 3200/20000]\n",
      "loss: 0.678324  [ 4800/20000]\n",
      "loss: 0.682849  [ 6400/20000]\n",
      "loss: 0.681510  [ 8000/20000]\n",
      "loss: 0.670889  [ 9600/20000]\n",
      "loss: 0.668815  [11200/20000]\n",
      "loss: 0.673859  [12800/20000]\n",
      "loss: 0.674163  [14400/20000]\n",
      "loss: 0.671526  [16000/20000]\n",
      "loss: 0.681370  [17600/20000]\n",
      "loss: 0.683250  [19200/20000]\n",
      "Train Loss: 0.6808 | Val Loss: 0.6956 | Val Acc: 51.44%\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.673727  [    0/20000]\n",
      "loss: 0.672617  [ 1600/20000]\n",
      "loss: 0.674110  [ 3200/20000]\n",
      "loss: 0.673148  [ 4800/20000]\n",
      "loss: 0.685202  [ 6400/20000]\n",
      "loss: 0.674440  [ 8000/20000]\n",
      "loss: 0.682449  [ 9600/20000]\n",
      "loss: 0.671763  [11200/20000]\n",
      "loss: 0.670115  [12800/20000]\n",
      "loss: 0.676641  [14400/20000]\n",
      "loss: 0.673652  [16000/20000]\n",
      "loss: 0.670916  [17600/20000]\n",
      "loss: 0.663181  [19200/20000]\n",
      "Train Loss: 0.6781 | Val Loss: 0.6984 | Val Acc: 49.74%\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.671315  [    0/20000]\n",
      "loss: 0.681092  [ 1600/20000]\n",
      "loss: 0.671242  [ 3200/20000]\n",
      "loss: 0.671398  [ 4800/20000]\n",
      "loss: 0.674136  [ 6400/20000]\n",
      "loss: 0.673204  [ 8000/20000]\n",
      "loss: 0.672612  [ 9600/20000]\n",
      "loss: 0.675349  [11200/20000]\n",
      "loss: 0.674142  [12800/20000]\n",
      "loss: 0.675516  [14400/20000]\n",
      "loss: 0.669836  [16000/20000]\n",
      "loss: 0.674432  [17600/20000]\n",
      "loss: 0.669585  [19200/20000]\n",
      "Train Loss: 0.6784 | Val Loss: 0.6986 | Val Acc: 49.74%\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.674560  [    0/20000]\n",
      "loss: 0.679184  [ 1600/20000]\n",
      "loss: 0.669766  [ 3200/20000]\n",
      "loss: 0.671824  [ 4800/20000]\n",
      "loss: 0.671577  [ 6400/20000]\n",
      "loss: 0.677788  [ 8000/20000]\n",
      "loss: 0.677488  [ 9600/20000]\n",
      "loss: 0.690028  [11200/20000]\n",
      "loss: 0.672773  [12800/20000]\n",
      "loss: 0.668610  [14400/20000]\n",
      "loss: 0.669633  [16000/20000]\n",
      "loss: 0.674327  [17600/20000]\n",
      "loss: 0.674171  [19200/20000]\n",
      "Train Loss: 0.6765 | Val Loss: 0.6988 | Val Acc: 49.60%\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.670984  [    0/20000]\n",
      "loss: 0.674241  [ 1600/20000]\n",
      "loss: 0.675809  [ 3200/20000]\n",
      "loss: 0.673672  [ 4800/20000]\n",
      "loss: 0.672077  [ 6400/20000]\n",
      "loss: 0.741107  [ 8000/20000]\n",
      "loss: 0.670527  [ 9600/20000]\n",
      "loss: 0.676351  [11200/20000]\n",
      "loss: 0.675800  [12800/20000]\n",
      "loss: 0.673039  [14400/20000]\n",
      "loss: 0.672879  [16000/20000]\n",
      "loss: 0.672942  [17600/20000]\n",
      "loss: 0.672156  [19200/20000]\n",
      "Train Loss: 0.6763 | Val Loss: 0.6962 | Val Acc: 51.34%\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.670569  [    0/20000]\n",
      "loss: 0.672288  [ 1600/20000]\n",
      "loss: 0.671475  [ 3200/20000]\n",
      "loss: 0.666925  [ 4800/20000]\n",
      "loss: 0.676786  [ 6400/20000]\n",
      "loss: 0.676583  [ 8000/20000]\n",
      "loss: 0.668954  [ 9600/20000]\n",
      "loss: 0.737850  [11200/20000]\n",
      "loss: 0.667843  [12800/20000]\n",
      "loss: 0.660383  [14400/20000]\n",
      "loss: 0.670178  [16000/20000]\n",
      "loss: 0.671742  [17600/20000]\n",
      "loss: 0.672411  [19200/20000]\n",
      "Train Loss: 0.6755 | Val Loss: 0.7023 | Val Acc: 51.18%\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    val_loss, val_acc = val_loop(val_loader, model, loss_fn)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {100*val_acc:.2f}%\\n\")\n",
    "print(\"Done!\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
